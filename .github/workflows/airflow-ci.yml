name: CI/CD for Airflow DAGs

on:
  push:
    branches:
      - main  # Run on push to main
      - DataPipeline1
  pull_request:
    branches:
      - main  # Run on PRs to main

jobs:
  test:
    name: Run Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.8'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r datapipeline/requirements.txt
          pip install pytest coverage  # For testing

      - name: Run Tests with Coverage
        run: |
          coverage run --source=datapipeline/scripts -m pytest datapipeline/tests/
          coverage report -m
          coverage xml  # Generate XML for code coverage reporting

  deploy:
    name: Deploy DAGs to Cloud Composer
    runs-on: ubuntu-latest
    needs: test  # Only run if tests pass

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Deploy DAGs to Cloud Composer
        run: |
          gsutil -m cp -r datapipeline/dags/extract_bigquery_schema.py gs://${{ secrets.CLOUD_COMPOSER_BUCKET }}/dags/
          gsutil -m cp -r datapipeline/scripts/schema_processor.py gs://${{ secrets.CLOUD_COMPOSER_BUCKET }}/dags/scripts/
